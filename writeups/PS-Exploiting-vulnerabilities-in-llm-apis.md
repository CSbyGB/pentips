# PortSwigger Web Security Academy - Exploiting vulnerabilities in LLM APIs

## Instructions

This lab contains an OS command injection vulnerability that can be exploited via its APIs. You can call these APIs via the LLM. To solve the lab, delete the morale.txt file from Carlos' home directory.

## Solving

When we launch the lab we get on this page:  
![Landing](../.res/2024-10-24-14-14-56.png)  

We need to click on Live Chat to access the AI Chatbot.  

First we can ask it which API it uses:  
![API](../.res/2024-10-24-14-25-30.png)  

Now we could ask more info about the newsletter for example:  
![Newsletter](../.res/2024-10-24-14-26-23.png)  

We can also try to add an address to the newsletter:  
![tests](../.res/2024-10-24-14-34-07.png)  

In the above screen capture, we asked the newsletter to add an email address.  
As we have access to an email client we can ask to be added.  
And then we can try to inject a command.  
Here we inject whoami and it works as we can see below.  

![cmd](../.res/2024-10-24-14-36-27.png)  

So as the goal is to delete  a specific morale.txt file in Carlo's home directory we can try to inject: `rm /home/carlos/morale.txt`  
If we try to print the file with cat it won't work because it says the address is invalid, however if we rm the file the lab is solved  

![remove](../.res/2024-10-24-14-40-06.png)  

And we solved this lab :)  

![Solved](../.res/2024-10-24-14-40-42.png)